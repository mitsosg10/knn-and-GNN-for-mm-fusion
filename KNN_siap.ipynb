{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQQ2fezNjp99K2SzFwWsY4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mitsosg10/knn-and-GNN-for-mm-fusion/blob/main/KNN_siap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importing necessary Libraries and packages**"
      ],
      "metadata": {
        "id": "EzNt5NVkQfYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kneed # install kneed package to automatically find optimal value for k (although computationaly expensive)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from kneed import KneeLocator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bauhrf6zQbz8",
        "outputId": "b1a29359-12cb-43db-d50e-0f339d03e0f3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kneed\n",
            "  Downloading kneed-0.8.5-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: numpy>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from kneed) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from kneed) (1.13.1)\n",
            "Downloading kneed-0.8.5-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: kneed\n",
            "Successfully installed kneed-0.8.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Uploading the data**"
      ],
      "metadata": {
        "id": "nBHFgjsgN1WO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "zfrwLCY1NV9E",
        "outputId": "ae1ab6d4-bbd5-451b-de70-32c755073f4f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-892c64e9-2304-4f24-855a-4e66fd134068\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-892c64e9-2304-4f24-855a-4e66fd134068\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving siap.xlsx to siap.xlsx\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel(io.BytesIO(uploaded['siap.xlsx']))\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "TBwVYqZ_NyWG",
        "outputId": "e4ec0b19-078b-4f97-969a-cfc1c1b6bdbd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id  crew_criminal_record  abnormal_route_detection  cargo_volume  \\\n",
              "0   0                     0                         0            90   \n",
              "1   1                     0                         0            90   \n",
              "2   2                     0                         0           100   \n",
              "3   3                     0                         0            95   \n",
              "4   4                     0                         0            95   \n",
              "\n",
              "               cargo_type  previous_violations  insurance_claims  \\\n",
              "0             Electronics                    7                 1   \n",
              "1                  Timber                    5                 0   \n",
              "2             Electronics                    8                 0   \n",
              "3             Electronics                    8                 0   \n",
              "4  Chemicals, Oil and Gas                    9                 0   \n",
              "\n",
              "  ship_condition  crew_size  ship_size  illegal_activity  port_call_frequency  \\\n",
              "0           Fair         16      12961                 0                   18   \n",
              "1           Poor         25      24107                 0                   27   \n",
              "2           Good         11      10027                 0                   47   \n",
              "3           Good         14      10372                 0                   71   \n",
              "4           Good         11      10581                 1                   26   \n",
              "\n",
              "   duration_at_port  inspection_history  \n",
              "0               204                  11  \n",
              "1               177                   5  \n",
              "2                15                   6  \n",
              "3                 6                  10  \n",
              "4               127                   7  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-50a9858c-9e4a-43e5-96da-4f7cc0e760af\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>crew_criminal_record</th>\n",
              "      <th>abnormal_route_detection</th>\n",
              "      <th>cargo_volume</th>\n",
              "      <th>cargo_type</th>\n",
              "      <th>previous_violations</th>\n",
              "      <th>insurance_claims</th>\n",
              "      <th>ship_condition</th>\n",
              "      <th>crew_size</th>\n",
              "      <th>ship_size</th>\n",
              "      <th>illegal_activity</th>\n",
              "      <th>port_call_frequency</th>\n",
              "      <th>duration_at_port</th>\n",
              "      <th>inspection_history</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>90</td>\n",
              "      <td>Electronics</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>Fair</td>\n",
              "      <td>16</td>\n",
              "      <td>12961</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>204</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>90</td>\n",
              "      <td>Timber</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>Poor</td>\n",
              "      <td>25</td>\n",
              "      <td>24107</td>\n",
              "      <td>0</td>\n",
              "      <td>27</td>\n",
              "      <td>177</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>100</td>\n",
              "      <td>Electronics</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>Good</td>\n",
              "      <td>11</td>\n",
              "      <td>10027</td>\n",
              "      <td>0</td>\n",
              "      <td>47</td>\n",
              "      <td>15</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>95</td>\n",
              "      <td>Electronics</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>Good</td>\n",
              "      <td>14</td>\n",
              "      <td>10372</td>\n",
              "      <td>0</td>\n",
              "      <td>71</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>95</td>\n",
              "      <td>Chemicals, Oil and Gas</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>Good</td>\n",
              "      <td>11</td>\n",
              "      <td>10581</td>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>127</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-50a9858c-9e4a-43e5-96da-4f7cc0e760af')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-50a9858c-9e4a-43e5-96da-4f7cc0e760af button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-50a9858c-9e4a-43e5-96da-4f7cc0e760af');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-af9e2629-7e2b-41a2-bafe-ebab9d7ac9d3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-af9e2629-7e2b-41a2-bafe-ebab9d7ac9d3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-af9e2629-7e2b-41a2-bafe-ebab9d7ac9d3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 100000,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2886,\n        \"min\": 0,\n        \"max\": 9999,\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          6252,\n          4684,\n          1731\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"crew_criminal_record\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abnormal_route_detection\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cargo_volume\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12,\n        \"min\": 50,\n        \"max\": 100,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          80,\n          90\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cargo_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Timber\",\n          \"Food\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"previous_violations\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 10,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          10,\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"insurance_claims\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ship_condition\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Fair\",\n          \"Poor\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"crew_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5,\n        \"min\": 8,\n        \"max\": 30,\n        \"num_unique_values\": 23,\n        \"samples\": [\n          26,\n          20\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ship_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17291,\n        \"min\": 8000,\n        \"max\": 60000,\n        \"num_unique_values\": 18596,\n        \"samples\": [\n          23280,\n          51541\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"illegal_activity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"port_call_frequency\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 19,\n        \"min\": 1,\n        \"max\": 100,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          85,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"duration_at_port\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 67,\n        \"min\": 1,\n        \"max\": 240,\n        \"num_unique_values\": 240,\n        \"samples\": [\n          20,\n          14\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"inspection_history\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 20,\n        \"num_unique_values\": 21,\n        \"samples\": [\n          11,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data preprocessing**"
      ],
      "metadata": {
        "id": "IVpoQzNdPQbd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Converting categorical variables to binary**"
      ],
      "metadata": {
        "id": "53JLAiu2OLlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#categorical to binary\n",
        "df_encoded = pd.get_dummies(df, columns=['cargo_type', 'ship_condition'], drop_first=True)"
      ],
      "metadata": {
        "id": "xpiNji1DOEk7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spliting the dataset**"
      ],
      "metadata": {
        "id": "T9dbL8jEOUb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df_encoded.drop('illegal_activity', axis=1)\n",
        "y = df_encoded['illegal_activity']\n",
        "\n",
        "# Split data: 60% train, 20% validation, 20% test\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "print(\"Training set size:\", X_train.shape)\n",
        "print(\"Validation set size:\", X_val.shape)\n",
        "print(\"Test set size:\", X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzrXhAzAOZEl",
        "outputId": "8037c765-70b7-4f6b-ecbf-b294a976e7e0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: (60000, 17)\n",
            "Validation set size: (20000, 17)\n",
            "Test set size: (20000, 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scaling the Data**\n",
        "\n",
        "We handle the numerical data by standardizing it using tools like StandardScaler to ensure the features have a mean of 0 and a standard deviation of 1. This ensures fair distance calculations in the KNN algorithm."
      ],
      "metadata": {
        "id": "2P4jbmxVP-Ac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "LYafvCeKQLZs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Deep learning**"
      ],
      "metadata": {
        "id": "LxCQRyPIPahl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KNN to Define Graph Edges**\n",
        "\n",
        "Use KNN to identify the nearest neighbors for each data point:\n",
        "\n",
        "Each data point becomes a node.\n",
        "Edges connect nodes to their K nearest neighbors. Firstly we will set k=5"
      ],
      "metadata": {
        "id": "2LqWoO9qPeiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# Apply KNN\n",
        "def create_knn_graph(X, n_neighbors):\n",
        "    knn = NearestNeighbors(n_neighbors=n_neighbors)\n",
        "    knn.fit(X)\n",
        "    distances, indices = knn.kneighbors(X)\n",
        "\n",
        "    # Create an edge list\n",
        "    edges = []\n",
        "    for node, neighbors in enumerate(indices):\n",
        "        for neighbor in neighbors:\n",
        "            if node != neighbor:  # Avoid self-loops\n",
        "                edges.append((node, neighbor))\n",
        "\n",
        "    return edges, distances\n",
        "\n",
        "# Using K=5\n",
        "n_neighbors = 5\n",
        "edges, distances = create_knn_graph(X_scaled, n_neighbors)"
      ],
      "metadata": {
        "id": "kyxH8rAkPqPl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimize k** (makes sense after GNN implimentation)"
      ],
      "metadata": {
        "id": "H5r7rtugRMjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_k(X, k_range):\n",
        "    \"\"\"\n",
        "    Find the optimal k value using the elbow method.\n",
        "\n",
        "    Args:\n",
        "        X (array-like): The data matrix.\n",
        "        k_range (iterable): A range of k values to consider.\n",
        "\n",
        "    Returns:\n",
        "        int: The optimal k value.\n",
        "    \"\"\"\n",
        "\n",
        "    distortions = []\n",
        "    for k in k_range:\n",
        "        knn = NearestNeighbors(n_neighbors=k)\n",
        "        knn.fit(X)\n",
        "        distances, _ = knn.kneighbors(X)\n",
        "        distortions.append(distances[:, -1].mean())  # Average distance to kth neighbor\n",
        "\n",
        "    # Use KneeLocator to find the elbow point\n",
        "    knee = KneeLocator(k_range, distortions, curve='convex', direction='increasing')\n",
        "    optimal_k = knee.elbow\n",
        "\n",
        "    return optimal_k\n",
        "\n",
        "# Example: Optimize for K in range 2 to 10\n",
        "optimal_k = optimize_k(X_scaled, range(2, 10)) # set range for k\n",
        "print(f\"Optimal K: {optimal_k}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_EOxsrsRXUT",
        "outputId": "67ea0e66-b1c3-4019-f5e1-2a83d833e765"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal K: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert to Graph Representation**\n"
      ],
      "metadata": {
        "id": "3R-eCHvmSI5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "# Create a graph\n",
        "def knn_to_graph(edges):\n",
        "    G = nx.Graph()\n",
        "    G.add_edges_from(edges)\n",
        "    return G\n",
        "\n",
        "graph = knn_to_graph(edges)\n",
        "\n",
        "# Adjacency matrix for GNN input\n",
        "adj_matrix = nx.adjacency_matrix(graph).toarray()\n",
        "\n",
        "print(adj_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRwdktLiUCTJ",
        "outputId": "bb912348-6dad-4f21-cf3f-9fc89b3d0a20"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 1 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **GNN**\n",
        "\n",
        "Having the adj matrix we can move to the graph neural network"
      ],
      "metadata": {
        "id": "G1F4APfMfI21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GNNs(object):\n",
        "    def __init__(self, adj_matrix, features, labels, tvt_nids, cuda=-1, hidden_size=64, n_layers=1, epochs=200, seed=-1, lr=1e-2, weight_decay=5e-4, dropout=0.5, log=True, name='debug', gnn='gcn'):\n",
        "        self.lr = lr\n",
        "        self.weight_decay = weight_decay\n",
        "        self.n_epochs = epochs\n",
        "        # create a logger, logs are saved to GNN-[name].log when name is not None\n",
        "        if log:\n",
        "            self.logger = self.get_logger(name)\n",
        "        else:\n",
        "            # disable logger if wanted\n",
        "            self.logger = logging.getLogger()\n",
        "        # config device (force device to cpu when cuda is not available)\n",
        "        if not torch.cuda.is_available():\n",
        "            cuda = -1\n",
        "        self.device = torch.device(f'cuda:{cuda}' if cuda>=0 else 'cpu')\n",
        "        # log all parameters to keep record\n",
        "        all_vars = locals()\n",
        "        self.log_parameters(all_vars)\n",
        "        # fix random seeds if needed\n",
        "        if seed > 0:\n",
        "            np.random.seed(seed)\n",
        "            torch.manual_seed(seed)\n",
        "            torch.cuda.manual_seed_all(seed)\n",
        "        # load data\n",
        "        self.load_data(adj_matrix, features, labels, tvt_nids, gnn)\n",
        "        # setup the model\n",
        "        self.model = GNN(self.features.size(1),\n",
        "                         hidden_size,\n",
        "                         self.n_class,\n",
        "                         n_layers,\n",
        "                         F.relu,\n",
        "                         dropout,\n",
        "                         gnnlayer_type=gnn)\n",
        "\n",
        "    def load_data(self, adj_matrix, features, labels, tvt_nids, gnnlayer_type):\n",
        "        \"\"\" preprocess data \"\"\"\n",
        "        # features (torch.FloatTensor)\n",
        "        if isinstance(features, torch.FloatTensor):\n",
        "            self.features = features\n",
        "        else:\n",
        "            self.features = torch.FloatTensor(features)\n",
        "        self.features = F.normalize(self.features, p=1, dim=1)\n",
        "        # original adj_matrix for training vgae (torch.FloatTensor)\n",
        "        assert sp.issparse(adj_matrix)\n",
        "        if not isinstance(adj_matrix, sp.coo_matrix):\n",
        "            adj_matrix = sp.coo_matrix(adj_matrix)\n",
        "        adj_matrix.setdiag(1)\n",
        "        # normalized adj_matrix (torch.sparse.FloatTensor)\n",
        "        if gnnlayer_type == 'gcn':\n",
        "            degrees = np.array(adj_matrix.sum(1))\n",
        "            degree_mat_inv_sqrt = sp.diags(np.power(degrees, -0.5).flatten())\n",
        "            adj_norm = degree_mat_inv_sqrt @ adj_matrix @ degree_mat_inv_sqrt\n",
        "            self.adj = scipysp_to_pytorchsp(adj_norm)\n",
        "        elif gnnlayer_type == 'gsage':\n",
        "            adj_matrix_noselfloop = sp.coo_matrix(adj_matrix)\n",
        "            # adj_matrix_noselfloop.setdiag(0)\n",
        "            # adj_matrix_noselfloop.eliminate_zeros()\n",
        "            adj_matrix_noselfloop = sp.coo_matrix(adj_matrix_noselfloop / adj_matrix_noselfloop.sum(1))\n",
        "            self.adj = scipysp_to_pytorchsp(adj_matrix_noselfloop)\n",
        "        elif gnnlayer_type == 'gat':\n",
        "            # self.adj = scipysp_to_pytorchsp(adj_matrix)\n",
        "            self.adj = torch.FloatTensor(adj_matrix.todense())\n",
        "        # labels (torch.LongTensor) and train/validation/test nids (np.ndarray)\n",
        "        if isinstance(labels, np.ndarray):\n",
        "            labels = torch.LongTensor(labels)\n",
        "        self.labels = labels\n",
        "        assert len(labels.size()) == 1\n",
        "        self.train_nid = tvt_nids[0]\n",
        "        self.val_nid = tvt_nids[1]\n",
        "        self.test_nid = tvt_nids[2]\n",
        "        # number of classes\n",
        "        self.n_class = len(torch.unique(self.labels))\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\" train the model \"\"\"\n",
        "        # move data to device\n",
        "        adj = self.adj.to(self.device)\n",
        "        features = self.features.to(self.device)\n",
        "        labels = self.labels.to(self.device)\n",
        "        model = self.model.to(self.device)\n",
        "        optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                     lr=self.lr,\n",
        "                                     weight_decay=self.weight_decay)\n",
        "        # keep record of the best validation accuracy for early stopping\n",
        "        best_val_acc = 0.\n",
        "        # train model\n",
        "        for epoch in range(self.n_epochs):\n",
        "            model.train()\n",
        "            nc_logits = model(adj, features)\n",
        "            # losses\n",
        "            loss = F.nll_loss(nc_logits[self.train_nid], labels[self.train_nid])\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # validate (without dropout)\n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                nc_logits_eval = model(adj, features)\n",
        "            val_acc = self.eval_node_cls(nc_logits_eval[self.val_nid], labels[self.val_nid])\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                test_acc = self.eval_node_cls(nc_logits_eval[self.test_nid], labels[self.test_nid])\n",
        "                self.logger.info('Epoch [{:3}/{}]: loss {:.4f}, val acc {:.4f}, test acc {:.4f}'\n",
        "                            .format(epoch+1, self.n_epochs, loss.item(), val_acc, test_acc))\n",
        "            else:\n",
        "                self.logger.info('Epoch [{:3}/{}]: loss {:.4f}, val acc {:.4f}'\n",
        "                            .format(epoch+1, self.n_epochs, loss.item(), val_acc))\n",
        "        # get final test result without early stop\n",
        "        with torch.no_grad():\n",
        "            nc_logits_eval = model(adj, features)\n",
        "        test_acc_final = self.eval_node_cls(nc_logits_eval[self.test_nid], labels[self.test_nid])\n",
        "        # log both results\n",
        "        self.logger.info('Final test acc with early stop: {:.4f}, without early stop: {:.4f}'\n",
        "                    .format(test_acc, test_acc_final))\n",
        "        return test_acc\n",
        "\n",
        "    def log_parameters(self, all_vars):\n",
        "        \"\"\" log all variables in the input dict excluding the following ones \"\"\"\n",
        "        del all_vars['self']\n",
        "        del all_vars['adj_matrix']\n",
        "        del all_vars['features']\n",
        "        del all_vars['labels']\n",
        "        del all_vars['tvt_nids']\n",
        "        self.logger.info(f'Parameters: {all_vars}')\n",
        "\n",
        "    @staticmethod\n",
        "    def eval_node_cls(nc_logits, labels):\n",
        "        \"\"\" evaluate node classification results \"\"\"\n",
        "        preds = torch.argmax(nc_logits, dim=1)\n",
        "        correct = torch.sum(preds == labels)\n",
        "        acc = correct.item() / len(labels)\n",
        "        return acc\n",
        "\n",
        "    @staticmethod\n",
        "    def get_logger(name):\n",
        "        \"\"\" create a nice logger \"\"\"\n",
        "        logger = logging.getLogger(name)\n",
        "        # clear handlers if they were created in other runs\n",
        "        if (logger.hasHandlers()):\n",
        "            logger.handlers.clear()\n",
        "        logger.setLevel(logging.DEBUG)\n",
        "        # create formatter\n",
        "        formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
        "        # create console handler add add to logger\n",
        "        ch = logging.StreamHandler()\n",
        "        ch.setLevel(logging.DEBUG)\n",
        "        ch.setFormatter(formatter)\n",
        "        logger.addHandler(ch)\n",
        "        # create file handler add add to logger when name is not None\n",
        "        if name is not None:\n",
        "            fh = logging.FileHandler(f'GAug-{name}.log')\n",
        "            fh.setFormatter(formatter)\n",
        "            fh.setLevel(logging.DEBUG)\n",
        "            logger.addHandler(fh)\n",
        "        return logger\n",
        "\n",
        "\n",
        "class GNN(nn.Module):\n",
        "    \"\"\" GNN as node classification model \"\"\"\n",
        "    def __init__(self, dim_feats, dim_h, n_classes, n_layers, activation, dropout, gnnlayer_type='gcn'):\n",
        "        super(GNN, self).__init__()\n",
        "        heads = [1] * (n_layers + 1)\n",
        "        if gnnlayer_type == 'gcn':\n",
        "            gnnlayer = GCNLayer\n",
        "        elif gnnlayer_type == 'gsage':\n",
        "            gnnlayer = SAGELayer\n",
        "        elif gnnlayer_type == 'gat':\n",
        "            gnnlayer = GATLayer\n",
        "            heads = [8] * n_layers + [1]\n",
        "            activation = F.elu\n",
        "        self.layers = nn.ModuleList()\n",
        "        # input layer\n",
        "        self.layers.append(gnnlayer(dim_feats, dim_h, heads[0], activation, 0))\n",
        "        # hidden layers\n",
        "        for i in range(n_layers - 1):\n",
        "            self.layers.append(gnnlayer(dim_h*heads[i], dim_h, heads[i+1], activation, dropout))\n",
        "        # output layer\n",
        "        self.layers.append(gnnlayer(dim_h*heads[-2], n_classes, heads[-1], None, dropout))\n",
        "\n",
        "    def forward(self, adj, features):\n",
        "        h = features\n",
        "        for layer in self.layers:\n",
        "            h = layer(adj, h)\n",
        "        return F.log_softmax(h, dim=1)\n",
        "\n",
        "\n",
        "class GCNLayer(nn.Module):\n",
        "    \"\"\" one layer of GCN \"\"\"\n",
        "    def __init__(self, input_dim, output_dim, n_heads, activation, dropout, bias=True):\n",
        "        super(GCNLayer, self).__init__()\n",
        "        self.W = nn.Parameter(torch.FloatTensor(input_dim, output_dim))\n",
        "        self.activation = activation\n",
        "        if bias:\n",
        "            self.b = nn.Parameter(torch.FloatTensor(output_dim))\n",
        "        else:\n",
        "            self.b = None\n",
        "        if dropout:\n",
        "            self.dropout = nn.Dropout(p=dropout)\n",
        "        else:\n",
        "            self.dropout = 0\n",
        "        self.init_params()\n",
        "\n",
        "    def init_params(self):\n",
        "        \"\"\" Initialize weights with xavier uniform and biases with all zeros \"\"\"\n",
        "        for param in self.parameters():\n",
        "            if len(param.size()) == 2:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "            else:\n",
        "                nn.init.constant_(param, 0.0)\n",
        "\n",
        "    def forward(self, adj, h):\n",
        "        if self.dropout:\n",
        "            h = self.dropout(h)\n",
        "        x = h @ self.W\n",
        "        x = adj @ x\n",
        "        if self.b is not None:\n",
        "            x = x + self.b\n",
        "        if self.activation:\n",
        "            x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SAGELayer(nn.Module):\n",
        "    \"\"\" one layer of GraphSAGE with gcn aggregator \"\"\"\n",
        "    def __init__(self, input_dim, output_dim, n_heads, activation, dropout, bias=False):\n",
        "        super(SAGELayer, self).__init__()\n",
        "        self.linear_neigh = nn.Linear(input_dim, output_dim, bias=False)\n",
        "        # self.linear_self = nn.Linear(input_dim, output_dim, bias=False)\n",
        "        self.activation = activation\n",
        "        if dropout:\n",
        "            self.dropout = nn.Dropout(p=dropout)\n",
        "        else:\n",
        "            self.dropout = 0\n",
        "        self.init_params()\n",
        "\n",
        "    def init_params(self):\n",
        "        \"\"\" Initialize weights with xavier uniform and biases with all zeros \"\"\"\n",
        "        for param in self.parameters():\n",
        "            if len(param.size()) == 2:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "            else:\n",
        "                nn.init.constant_(param, 0.0)\n",
        "\n",
        "    def forward(self, adj, h):\n",
        "        if self.dropout:\n",
        "            h = self.dropout(h)\n",
        "        x = adj @ h\n",
        "        x = self.linear_neigh(x)\n",
        "        # x_neigh = self.linear_neigh(x)\n",
        "        # x_self = self.linear_self(h)\n",
        "        # x = x_neigh + x_self\n",
        "        if self.activation:\n",
        "            x = self.activation(x)\n",
        "        # x = F.normalize(x, dim=1, p=2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GATLayer(nn.Module):\n",
        "    \"\"\" one layer of GAT \"\"\"\n",
        "    def __init__(self, input_dim, output_dim, n_heads, activation, dropout, bias=True):\n",
        "        super(GATLayer, self).__init__()\n",
        "        self.W = nn.Parameter(torch.FloatTensor(input_dim, output_dim))\n",
        "        self.activation = activation\n",
        "        self.n_heads = n_heads\n",
        "        self.attn_l = nn.Linear(output_dim, self.n_heads, bias=False)\n",
        "        self.attn_r = nn.Linear(output_dim, self.n_heads, bias=False)\n",
        "        if dropout:\n",
        "            self.dropout = nn.Dropout(p=dropout)\n",
        "        else:\n",
        "            self.dropout = 0\n",
        "        if bias:\n",
        "            self.b = nn.Parameter(torch.FloatTensor(output_dim))\n",
        "        else:\n",
        "            self.b = None\n",
        "        self.init_params()\n",
        "\n",
        "    def init_params(self):\n",
        "        \"\"\" Initialize weights with xavier uniform and biases with all zeros \"\"\"\n",
        "        for param in self.parameters():\n",
        "            if len(param.size()) == 2:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "            else:\n",
        "                nn.init.constant_(param, 0.0)\n",
        "\n",
        "    def forward(self, adj, h):\n",
        "        if self.dropout:\n",
        "            h = self.dropout(h)\n",
        "        x = h @ self.W # torch.Size([2708, 128])\n",
        "        # calculate attentions, both el and er are n_nodes by n_heads\n",
        "        el = self.attn_l(x)\n",
        "        er = self.attn_r(x) # torch.Size([2708, 8])\n",
        "        if isinstance(adj, torch.sparse.FloatTensor):\n",
        "            nz_indices = adj._indices()\n",
        "        else:\n",
        "            nz_indices = adj.nonzero().T\n",
        "        attn = el[nz_indices[0]] + er[nz_indices[1]] # torch.Size([13264, 8])\n",
        "        attn = F.leaky_relu(attn, negative_slope=0.2).squeeze()\n",
        "        # reconstruct adj with attentions, exp for softmax next\n",
        "        attn = torch.exp(attn) # torch.Size([13264, 8]) NOTE: torch.Size([13264]) when n_heads=1\n",
        "        if self.n_heads == 1:\n",
        "            adj_attn = torch.zeros(size=(adj.size(0), adj.size(1)), device=adj.device)\n",
        "            adj_attn.index_put_((nz_indices[0], nz_indices[1]), attn)\n",
        "        else:\n",
        "            adj_attn = torch.zeros(size=(adj.size(0), adj.size(1), self.n_heads), device=adj.device)\n",
        "            adj_attn.index_put_((nz_indices[0], nz_indices[1]), attn) # torch.Size([2708, 2708, 8])\n",
        "            adj_attn.transpose_(1, 2) # torch.Size([2708, 8, 2708])\n",
        "        # edge softmax (only softmax with non-zero entries)\n",
        "        adj_attn = F.normalize(adj_attn, p=1, dim=-1)\n",
        "        # message passing\n",
        "        x = adj_attn @ x # torch.Size([2708, 8, 128])\n",
        "        if self.b is not None:\n",
        "            x = x + self.b\n",
        "        if self.activation:\n",
        "            x = self.activation(x)\n",
        "        if self.n_heads > 1:\n",
        "            x = x.flatten(start_dim=1)\n",
        "        return x # torch.Size([2708, 1024])\n",
        "\n",
        "\n",
        "def scipysp_to_pytorchsp(sp_mx):\n",
        "    \"\"\" converts scipy sparse matrix to pytorch sparse matrix \"\"\"\n",
        "    if not sp.isspmatrix_coo(sp_mx):\n",
        "        sp_mx = sp_mx.tocoo()\n",
        "    coords = np.vstack((sp_mx.row, sp_mx.col)).transpose()\n",
        "    values = sp_mx.data\n",
        "    shape = sp_mx.shape\n",
        "    pyt_sp_mx = torch.sparse.FloatTensor(torch.LongTensor(coords.T),\n",
        "                                         torch.FloatTensor(values),\n",
        "                                         torch.Size(shape))\n",
        "    return pyt_sp_mx"
      ],
      "metadata": {
        "id": "S290GsqmlIts"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import math\n",
        "import copy\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "class GNN(object):\n",
        "    \"\"\"Graph Neural Networks that can be easily called and used.\n",
        "\n",
        "    Authors of this code package:\n",
        "    Tong Zhao, tzhao2@nd.edu\n",
        "    Tianwen Jiang, twjiang@ir.hit.edu.cn\n",
        "\n",
        "    Last updated: 11/25/2019\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    adj_matrix: scipy.sparse.csr_matrix\n",
        "        The adjacency matrix of the graph, where nonzero entries indicates edges.\n",
        "        The number of each nonzero entry indicates the number of edges between these two nodes.\n",
        "\n",
        "    features: numpy.ndarray, optional\n",
        "        The 2-dimension np array that stores given raw feature of each node, where the i-th row\n",
        "        is the raw feature vector of node i.\n",
        "        When raw features are not given, one-hot degree features will be used.\n",
        "\n",
        "    labels: list or 1-D numpy.ndarray, optional\n",
        "        The class label of each node. Used for supervised learning.\n",
        "\n",
        "    supervised: bool, optional, default False\n",
        "        Whether to use supervised learning.\n",
        "\n",
        "    model: {'gat', 'graphsage'}, default 'gat'\n",
        "        The GNN model to be used.\n",
        "        - 'graphsage' is GraphSAGE: https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf\n",
        "        - 'gat' is graph attention network: https://arxiv.org/pdf/1710.10903.pdf\n",
        "\n",
        "    n_layer: int, optional, default 2\n",
        "        Number of layers in the GNN\n",
        "\n",
        "    emb_size: int, optional, default 128\n",
        "        Size of the node embeddings to be learnt\n",
        "\n",
        "    random_state, int, optional, default 1234\n",
        "        Random seed\n",
        "\n",
        "    device: {'cpu', 'cuda', 'auto'}, default 'auto'\n",
        "        The device to use.\n",
        "\n",
        "    epochs: int, optional, default 5\n",
        "        Number of epochs for training\n",
        "\n",
        "    batch_size: int, optional, default 20\n",
        "        Number of node per batch for training\n",
        "\n",
        "    lr: float, optional, default 0.7\n",
        "        Learning rate\n",
        "\n",
        "    unsup_loss_type: {'margin', 'normal'}, default 'margin'\n",
        "        Loss function to be used for unsupervised learning\n",
        "        - 'margin' is a hinge loss with margin of 3\n",
        "        - 'normal' is the unsupervised loss function described in the paper of GraphSAGE\n",
        "\n",
        "    print_progress: bool, optional, default True\n",
        "        Whether to print the training progress\n",
        "    \"\"\"\n",
        "    def __init__(self, adj_matrix, features=None, labels=None, supervised=False, model='gat', n_layer=2, emb_size=128, random_state=1234, device='auto', epochs=5, batch_size=20, lr=0.7, unsup_loss_type='margin', print_progress=True):\n",
        "        super(GNN, self).__init__()\n",
        "        # fix random seeds\n",
        "        random.seed(random_state)\n",
        "        np.random.seed(random_state)\n",
        "        torch.manual_seed(random_state)\n",
        "        torch.cuda.manual_seed_all(random_state)\n",
        "        # set parameters\n",
        "        self.supervised = supervised\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.unsup_loss_type = unsup_loss_type\n",
        "        self.print_progress = print_progress\n",
        "        self.gat = False\n",
        "        self.gcn = False\n",
        "        if model == 'gat':\n",
        "            self.gat = True\n",
        "            self.model_name = 'GAT'\n",
        "        elif model == 'gcn':\n",
        "            self.gcn = True\n",
        "            self.model_name = 'GCN'\n",
        "        else:\n",
        "            self.model_name = 'GraphSAGE'\n",
        "        # set device\n",
        "        if device == 'auto':\n",
        "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        else:\n",
        "            self.device = device\n",
        "\n",
        "        # load data\n",
        "        self.dl = DataLoader(adj_matrix, features, labels, supervised, self.device)\n",
        "\n",
        "        self.gnn = GNN_model(n_layer, emb_size, self.dl, self.device, gat=self.gat, gcn=self.gcn)\n",
        "        self.gnn.to(self.device)\n",
        "\n",
        "        if supervised:\n",
        "            n_classes = len(set(labels))\n",
        "            self.classification = Classification(emb_size, n_classes)\n",
        "            self.classification.to(self.device)\n",
        "\n",
        "    def fit(self):\n",
        "        train_nodes = copy.deepcopy(self.dl.nodes_train)\n",
        "\n",
        "        if self.supervised:\n",
        "            labels = self.dl.labels\n",
        "            models = [self.gnn, self.classification]\n",
        "        else:\n",
        "            unsup_loss = Unsup_Loss(self.dl, self.device)\n",
        "            models = [self.gnn]\n",
        "            if self.unsup_loss_type == 'margin':\n",
        "                num_neg = 6\n",
        "            elif self.unsup_loss_type == 'normal':\n",
        "                num_neg = 100\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            np.random.shuffle(train_nodes)\n",
        "\n",
        "            params = []\n",
        "            for model in models:\n",
        "                for param in model.parameters():\n",
        "                    if param.requires_grad:\n",
        "                        params.append(param)\n",
        "            optimizer = torch.optim.SGD(params, lr=self.lr)\n",
        "            optimizer.zero_grad()\n",
        "            for model in models:\n",
        "                model.zero_grad()\n",
        "\n",
        "            batches = math.ceil(len(train_nodes) / self.batch_size)\n",
        "            visited_nodes = set()\n",
        "            if self.print_progress:\n",
        "                tqdm_bar = tqdm(range(batches), ascii=True, leave=False)\n",
        "            else:\n",
        "                tqdm_bar = range(batches)\n",
        "            for index in tqdm_bar:\n",
        "                if not self.supervised and len(visited_nodes) == len(train_nodes):\n",
        "                    # finish this epoch if all nodes are visited\n",
        "                    if self.print_progress:\n",
        "                        tqdm_bar.close()\n",
        "                    break\n",
        "                nodes_batch = train_nodes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "                # extend nodes batch for unspervised learning\n",
        "                if not self.supervised:\n",
        "                    nodes_batch = np.asarray(list(unsup_loss.extend_nodes(nodes_batch, num_neg=num_neg)))\n",
        "                visited_nodes |= set(nodes_batch)\n",
        "                # feed nodes batch to the GNN and returning the nodes embeddings\n",
        "                embs_batch = self.gnn(nodes_batch)\n",
        "                # calculate loss\n",
        "                if self.supervised:\n",
        "                    # superivsed learning\n",
        "                    logists = self.classification(embs_batch)\n",
        "                    labels_batch = labels[nodes_batch]\n",
        "                    loss_sup = -torch.sum(logists[range(logists.size(0)), labels_batch], 0)\n",
        "                    loss_sup /= len(nodes_batch)\n",
        "                    loss = loss_sup\n",
        "                else:\n",
        "                    # unsupervised learning\n",
        "                    if self.unsup_loss_type == 'margin':\n",
        "                        loss_net = unsup_loss.get_loss_margin(embs_batch, nodes_batch)\n",
        "                    elif self.unsup_loss_type == 'normal':\n",
        "                        loss_net = unsup_loss.get_loss_sage(embs_batch, nodes_batch)\n",
        "                    loss = loss_net\n",
        "\n",
        "                if self.print_progress:\n",
        "                    progress_message = '{} Epoch: [{}/{}], current loss: {:.4f}, touched nodes [{}/{}] '.format(\n",
        "                                    self.model_name, epoch+1, self.epochs, loss.item(), len(visited_nodes), len(train_nodes))\n",
        "                    tqdm_bar.set_description(progress_message)\n",
        "\n",
        "                loss.backward()\n",
        "                for model in models:\n",
        "                    nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                for model in models:\n",
        "                    model.zero_grad()\n",
        "\n",
        "    def generate_embeddings(self):\n",
        "        nodes = self.dl.nodes_train\n",
        "        b_sz = 500\n",
        "        batches = math.ceil(len(nodes) / b_sz)\n",
        "        embs = []\n",
        "        for index in range(batches):\n",
        "            nodes_batch = nodes[index*b_sz:(index+1)*b_sz]\n",
        "            with torch.no_grad():\n",
        "                embs_batch = self.gnn(nodes_batch)\n",
        "            assert len(embs_batch) == len(nodes_batch)\n",
        "            embs.append(embs_batch)\n",
        "        assert len(embs) == batches\n",
        "        embs = torch.cat(embs, 0)\n",
        "        assert len(embs) == len(nodes)\n",
        "        return embs.cpu().numpy()\n",
        "\n",
        "    def predict(self):\n",
        "        if not self.supervised:\n",
        "            print('GNN.predict() is only supported for supervised learning.')\n",
        "            sys.exit(0)\n",
        "        nodes = self.dl.nodes_train\n",
        "        b_sz = 500\n",
        "        batches = math.ceil(len(nodes) / b_sz)\n",
        "        preds = []\n",
        "        for index in range(batches):\n",
        "            nodes_batch = nodes[index*b_sz:(index+1)*b_sz]\n",
        "            with torch.no_grad():\n",
        "                embs_batch = self.gnn(nodes_batch)\n",
        "                logists = self.classification(embs_batch)\n",
        "                _, predicts = torch.max(logists, 1)\n",
        "                preds.append(predicts)\n",
        "        assert len(preds) == batches\n",
        "        preds = torch.cat(preds, 0)\n",
        "        assert len(preds) == len(nodes)\n",
        "        return preds.cpu().numpy()\n",
        "\n",
        "    def release_cuda_cache(self):\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "class DataLoader(object):\n",
        "    def __init__(self, adj_matrix, raw_features, labels, supervised, device):\n",
        "        super(DataLoader, self).__init__()\n",
        "        self.adj_matrix = adj_matrix\n",
        "        # load adjacency list and node features\n",
        "        self.adj_list = self.get_adj_list(adj_matrix)\n",
        "        if raw_features is None:\n",
        "            features = self.get_features()\n",
        "        else:\n",
        "            features = raw_features\n",
        "        assert features.shape[0] == len(self.adj_list) == self.adj_matrix.shape[0]\n",
        "        self.features = torch.FloatTensor(features).to(device)\n",
        "        self.nodes_train = list(range(len(self.adj_list)))\n",
        "        if supervised:\n",
        "            self.labels = np.asarray(labels)\n",
        "\n",
        "    def get_adj_list(self, adj_matrix):\n",
        "        \"\"\"build adjacency list from adjacency matrix\"\"\"\n",
        "        adj_list = {}\n",
        "        for i in range(adj_matrix.shape[0]):\n",
        "            adj_list[i] = set(np.where(adj_matrix[i].toarray() != 0)[1])\n",
        "        return adj_list\n",
        "\n",
        "    def get_features(self):\n",
        "        \"\"\"\n",
        "        When raw features are not available,\n",
        "        build one-hot degree features from the adjacency list.\n",
        "        \"\"\"\n",
        "        max_degree = np.max(np.sum(self.adj_matrix != 0, axis=1))\n",
        "        features = np.zeros((self.adj_matrix.shape[0], max_degree))\n",
        "        for node, neighbors in self.adj_list.items():\n",
        "            features[node, len(neighbors)-1] = 1\n",
        "        return features\n",
        "\n",
        "\n",
        "class Classification(nn.Module):\n",
        "    def __init__(self, emb_size, num_classes):\n",
        "        super(Classification, self).__init__()\n",
        "        self.fc1 = nn.Linear(emb_size, 64)\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, embeds):\n",
        "        x = F.elu(self.fc1(embeds))\n",
        "        x = F.elu(self.fc2(x))\n",
        "        logists = torch.log_softmax(x, 1)\n",
        "        return logists\n",
        "\n",
        "\n",
        "class Unsup_Loss(object):\n",
        "    \"\"\"docstring for UnsupervisedLoss\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        super(Unsup_Loss, self).__init__()\n",
        "        self.Q = 10\n",
        "        self.N_WALKS = 4\n",
        "        self.WALK_LEN = 4\n",
        "        self.N_WALK_LEN = 5\n",
        "        self.MARGIN = 3\n",
        "        self.adj_lists = dl.adj_list\n",
        "        self.adj_matrix = dl.adj_matrix\n",
        "        self.train_nodes = dl.nodes_train\n",
        "        self.device = device\n",
        "\n",
        "        self.target_nodes = None\n",
        "        self.positive_pairs = []\n",
        "        self.negative_pairs = []\n",
        "        self.node_positive_pairs = {}\n",
        "        self.node_negative_pairs = {}\n",
        "        self.unique_nodes_batch = []\n",
        "\n",
        "    def get_loss_sage(self, embeddings, nodes):\n",
        "        assert len(embeddings) == len(self.unique_nodes_batch)\n",
        "        assert False not in [nodes[i]==self.unique_nodes_batch[i] for i in range(len(nodes))]\n",
        "        node2index = {n:i for i,n in enumerate(self.unique_nodes_batch)}\n",
        "\n",
        "        nodes_score = []\n",
        "        assert len(self.node_positive_pairs) == len(self.node_negative_pairs)\n",
        "        for node in self.node_positive_pairs:\n",
        "            pps = self.node_positive_pairs[node]\n",
        "            nps = self.node_negative_pairs[node]\n",
        "            if len(pps) == 0 or len(nps) == 0:\n",
        "                continue\n",
        "\n",
        "            # Q * Exception(negative score)\n",
        "            indexs = [list(x) for x in zip(*nps)]\n",
        "            node_indexs = [node2index[x] for x in indexs[0]]\n",
        "            neighb_indexs = [node2index[x] for x in indexs[1]]\n",
        "            neg_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n",
        "            neg_score = self.Q*torch.mean(torch.log(torch.sigmoid(-neg_score)), 0)\n",
        "\n",
        "            # multiple positive score\n",
        "            indexs = [list(x) for x in zip(*pps)]\n",
        "            node_indexs = [node2index[x] for x in indexs[0]]\n",
        "            neighb_indexs = [node2index[x] for x in indexs[1]]\n",
        "            pos_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n",
        "            pos_score = torch.log(torch.sigmoid(pos_score))\n",
        "\n",
        "            nodes_score.append(torch.mean(- pos_score - neg_score).view(1,-1))\n",
        "\n",
        "        loss = torch.mean(torch.cat(nodes_score, 0))\n",
        "        return loss\n",
        "\n",
        "    def get_loss_margin(self, embeddings, nodes):\n",
        "        assert len(embeddings) == len(self.unique_nodes_batch)\n",
        "        assert False not in [nodes[i]==self.unique_nodes_batch[i] for i in range(len(nodes))]\n",
        "        node2index = {n:i for i,n in enumerate(self.unique_nodes_batch)}\n",
        "\n",
        "        nodes_score = []\n",
        "        assert len(self.node_positive_pairs) == len(self.node_negative_pairs)\n",
        "        for node in self.node_positive_pairs:\n",
        "            pps = self.node_positive_pairs[node]\n",
        "            nps = self.node_negative_pairs[node]\n",
        "            if len(pps) == 0 or len(nps) == 0:\n",
        "                continue\n",
        "\n",
        "            indexs = [list(x) for x in zip(*pps)]\n",
        "            node_indexs = [node2index[x] for x in indexs[0]]\n",
        "            neighb_indexs = [node2index[x] for x in indexs[1]]\n",
        "            pos_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n",
        "            pos_score, _ = torch.min(torch.log(torch.sigmoid(pos_score)), 0)\n",
        "\n",
        "            indexs = [list(x) for x in zip(*nps)]\n",
        "            node_indexs = [node2index[x] for x in indexs[0]]\n",
        "            neighb_indexs = [node2index[x] for x in indexs[1]]\n",
        "            neg_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n",
        "            neg_score, _ = torch.max(torch.log(torch.sigmoid(neg_score)), 0)\n",
        "\n",
        "            nodes_score.append(torch.max(torch.tensor(0.0).to(self.device),\n",
        "                                         neg_score-pos_score+self.MARGIN).view(1, -1))\n",
        "        loss = torch.mean(torch.cat(nodes_score, 0), 0)\n",
        "        return loss\n",
        "\n",
        "    def extend_nodes(self, nodes, num_neg=6):\n",
        "        self.positive_pairs = []\n",
        "        self.node_positive_pairs = {}\n",
        "        self.negative_pairs = []\n",
        "        self.node_negative_pairs = {}\n",
        "\n",
        "        self.target_nodes = nodes\n",
        "        self.get_positive_nodes(nodes)\n",
        "        self.get_negative_nodes(nodes, num_neg)\n",
        "        self.unique_nodes_batch = list(set([i for x in self.positive_pairs for i in x])\n",
        "                                       | set([i for x in self.negative_pairs for i in x]))\n",
        "        assert set(self.target_nodes) < set(self.unique_nodes_batch)\n",
        "        return self.unique_nodes_batch\n",
        "\n",
        "    def get_positive_nodes(self, nodes):\n",
        "        return self._run_random_walks(nodes)\n",
        "\n",
        "    def get_negative_nodes(self, nodes, num_neg):\n",
        "        for node in nodes:\n",
        "            neighbors = set([node])\n",
        "            frontier = set([node])\n",
        "            for _ in range(self.N_WALK_LEN):\n",
        "                current = set()\n",
        "                for outer in frontier:\n",
        "                    current |= self.adj_lists[int(outer)]\n",
        "                frontier = current - neighbors\n",
        "                neighbors |= current\n",
        "            far_nodes = set(self.train_nodes) - neighbors\n",
        "            neg_samples = random.sample(far_nodes, num_neg) if num_neg < len(far_nodes) else far_nodes\n",
        "            self.negative_pairs.extend([(node, neg_node) for neg_node in neg_samples])\n",
        "            self.node_negative_pairs[node] = [(node, neg_node) for neg_node in neg_samples]\n",
        "        return self.negative_pairs\n",
        "\n",
        "    def _run_random_walks(self, nodes):\n",
        "        for node in nodes:\n",
        "            if len(self.adj_lists[int(node)]) == 0:\n",
        "                continue\n",
        "            cur_pairs = []\n",
        "            for _ in range(self.N_WALKS):\n",
        "                curr_node = node\n",
        "                for _ in range(self.WALK_LEN):\n",
        "                    cnts = self.adj_matrix[int(curr_node)].toarray().squeeze()\n",
        "                    neighs = []\n",
        "                    for n in np.where(cnts != 0)[0]:\n",
        "                        neighs.extend([n] * int(cnts[n]))\n",
        "                    # neighs = self.adj_lists[int(curr_node)]\n",
        "                    next_node = random.choice(list(neighs))\n",
        "                    # self co-occurrences are useless\n",
        "                    if next_node != node and next_node in self.train_nodes:\n",
        "                        self.positive_pairs.append((node,next_node))\n",
        "                        cur_pairs.append((node,next_node))\n",
        "                    curr_node = next_node\n",
        "\n",
        "            self.node_positive_pairs[node] = cur_pairs\n",
        "        return self.positive_pairs\n",
        "\n",
        "\n",
        "class SageLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Encodes a node's using 'convolutional' GraphSage approach\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, out_size, gat=False, gcn=False):\n",
        "        super(SageLayer, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.out_size = out_size\n",
        "\n",
        "        self.gat = gat\n",
        "        self.gcn = gcn\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(out_size, self.input_size if self.gat or self.gcn else 2 * self.input_size))\n",
        "\n",
        "        self.init_params()\n",
        "\n",
        "    def init_params(self):\n",
        "        for param in self.parameters():\n",
        "            nn.init.xavier_uniform_(param)\n",
        "\n",
        "    def forward(self, self_feats, aggregate_feats):\n",
        "        \"\"\"\n",
        "        Generates embeddings for a batch of nodes.\n",
        "        nodes\t -- list of nodes\n",
        "        \"\"\"\n",
        "        if self.gat or self.gcn:\n",
        "            combined = aggregate_feats\n",
        "        else:\n",
        "            combined = torch.cat([self_feats, aggregate_feats], dim=1)\n",
        "        combined = F.relu(self.weight.mm(combined.t())).t()\n",
        "        return combined\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"Computes the self-attention between pair of nodes\"\"\"\n",
        "    def __init__(self, input_size, out_size):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.out_size = out_size\n",
        "        self.attention_raw = nn.Linear(2*input_size, 1, bias=False)\n",
        "        self.attention_emb = nn.Linear(2*out_size, 1, bias=False)\n",
        "\n",
        "    def forward(self, row_embs, col_embs):\n",
        "        if row_embs.size(1) == self.input_size:\n",
        "            att = self.attention_raw\n",
        "        elif row_embs.size(1) == self.out_size:\n",
        "            att = self.attention_emb\n",
        "        e = att(torch.cat((row_embs, col_embs), dim=1))\n",
        "        return F.leaky_relu(e, negative_slope=0.2)\n",
        "\n",
        "class GNN_model(nn.Module):\n",
        "    \"\"\"docstring for GraphSage\"\"\"\n",
        "    def __init__(self, num_layers, out_size, dl, device, gat=False, gcn=False, agg_func='MEAN'):\n",
        "        super(GNN_model, self).__init__()\n",
        "\n",
        "        self.input_size = dl.features.size(1)\n",
        "        self.out_size = out_size\n",
        "        self.num_layers = num_layers\n",
        "        self.gat = gat\n",
        "        self.gcn = gcn\n",
        "        self.device = device\n",
        "        self.agg_func = agg_func\n",
        "\n",
        "        self.raw_features = dl.features\n",
        "        self.adj_lists = dl.adj_list\n",
        "        self.adj_matrix = dl.adj_matrix\n",
        "\n",
        "        for index in range(1, num_layers+1):\n",
        "            layer_size = out_size if index != 1 else self.input_size\n",
        "            setattr(self, 'sage_layer'+str(index), SageLayer(layer_size, out_size, gat=self.gat, gcn=self.gcn))\n",
        "        if self.gat:\n",
        "            self.attention = Attention(self.input_size, out_size)\n",
        "\n",
        "    def forward(self, nodes_batch):\n",
        "        \"\"\"\n",
        "        Generates embeddings for a batch of nodes.\n",
        "        nodes_batch\t-- batch of nodes to learn the embeddings\n",
        "        \"\"\"\n",
        "        lower_layer_nodes = list(nodes_batch)\n",
        "        nodes_batch_layers = [(lower_layer_nodes,)]\n",
        "        for _ in range(self.num_layers):\n",
        "            lower_layer_nodes, lower_samp_neighs, lower_layer_nodes_dict= self._get_unique_neighs_list(lower_layer_nodes)\n",
        "            nodes_batch_layers.insert(0, (lower_layer_nodes, lower_samp_neighs, lower_layer_nodes_dict))\n",
        "\n",
        "        assert len(nodes_batch_layers) == self.num_layers + 1\n",
        "\n",
        "        pre_hidden_embs = self.raw_features\n",
        "        for index in range(1, self.num_layers+1):\n",
        "            nb = nodes_batch_layers[index][0]\n",
        "            pre_neighs = nodes_batch_layers[index-1]\n",
        "            aggregate_feats = self.aggregate(nb, pre_hidden_embs, pre_neighs)\n",
        "            sage_layer = getattr(self, 'sage_layer'+str(index))\n",
        "            if index > 1:\n",
        "                nb = self._nodes_map(nb, pre_neighs)\n",
        "            cur_hidden_embs = sage_layer(self_feats=pre_hidden_embs[nb], aggregate_feats=aggregate_feats)\n",
        "            pre_hidden_embs = cur_hidden_embs\n",
        "\n",
        "        return pre_hidden_embs\n",
        "\n",
        "    def _nodes_map(self, nodes, neighs):\n",
        "        _, samp_neighs, layer_nodes_dict = neighs\n",
        "        assert len(samp_neighs) == len(nodes)\n",
        "        index = [layer_nodes_dict[x] for x in nodes]\n",
        "        return index\n",
        "\n",
        "    def _get_unique_neighs_list(self, nodes, num_sample=10):\n",
        "        _set = set\n",
        "        to_neighs = [self.adj_lists[int(node)] for node in nodes]\n",
        "        if self.gcn or self.gat:\n",
        "            samp_neighs = to_neighs\n",
        "        else:\n",
        "            _sample = random.sample\n",
        "            samp_neighs = [_set(_sample(to_neigh, num_sample)) if len(to_neigh) >= num_sample else to_neigh for to_neigh in to_neighs]\n",
        "        samp_neighs = [samp_neigh | set([nodes[i]]) for i, samp_neigh in enumerate(samp_neighs)]\n",
        "        _unique_nodes_list = list(set.union(*samp_neighs))\n",
        "        i = list(range(len(_unique_nodes_list)))\n",
        "        # unique node 2 index\n",
        "        unique_nodes = dict(list(zip(_unique_nodes_list, i)))\n",
        "        return _unique_nodes_list, samp_neighs, unique_nodes\n",
        "\n",
        "    def aggregate(self, nodes, pre_hidden_embs, pre_neighs):\n",
        "        unique_nodes_list, samp_neighs, unique_nodes = pre_neighs\n",
        "\n",
        "        assert len(nodes) == len(samp_neighs)\n",
        "        indicator = [(nodes[i] in samp_neighs[i]) for i in range(len(samp_neighs))]\n",
        "        assert False not in indicator\n",
        "        if not self.gat and not self.gcn:\n",
        "            samp_neighs = [(samp_neighs[i]-set([nodes[i]])) for i in range(len(samp_neighs))]\n",
        "        if len(pre_hidden_embs) == len(unique_nodes):\n",
        "            embed_matrix = pre_hidden_embs\n",
        "        else:\n",
        "            embed_matrix = pre_hidden_embs[torch.LongTensor(unique_nodes_list)]\n",
        "        # get row and column nonzero indices for the mask tensor\n",
        "        row_indices = [i for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))]\n",
        "        column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]\n",
        "        # get the edge counts for each edge\n",
        "        edge_counts = self.adj_matrix[nodes][:, unique_nodes_list].toarray()\n",
        "        edge_counts = torch.FloatTensor(edge_counts).to(embed_matrix.device)\n",
        "        torch.sqrt_(edge_counts)\n",
        "        if self.gat:\n",
        "            indices = (torch.LongTensor(row_indices), torch.LongTensor(column_indices))\n",
        "            nodes_indices = torch.LongTensor([unique_nodes[nodes[n]] for n in row_indices])\n",
        "            row_embs = embed_matrix[nodes_indices]\n",
        "            col_embs = embed_matrix[column_indices]\n",
        "            atts = self.attention(row_embs, col_embs).squeeze()\n",
        "            mask = torch.zeros(len(samp_neighs), len(unique_nodes)).to(embed_matrix.device)\n",
        "            mask.index_put_(indices, atts)\n",
        "            mask = mask * edge_counts\n",
        "            # softmax\n",
        "            mask = torch.exp(mask) * (mask != 0).float()\n",
        "            mask = F.normalize(mask, p=1, dim=1)\n",
        "        else:\n",
        "            mask = torch.zeros(len(samp_neighs), len(unique_nodes)).to(embed_matrix.device)\n",
        "            mask[row_indices, column_indices] = 1\n",
        "            # multiply edge counts to mask\n",
        "            mask = mask * edge_counts\n",
        "            mask = F.normalize(mask, p=1, dim=1)\n",
        "            mask = mask.to(embed_matrix.device)\n",
        "\n",
        "        if self.agg_func == 'MEAN':\n",
        "            aggregate_feats = mask.mm(embed_matrix)\n",
        "        elif self.agg_func == 'MAX':\n",
        "            indexs = [x.nonzero() for x in mask != 0]\n",
        "            aggregate_feats = []\n",
        "            for feat in [embed_matrix[x.squeeze()] for x in indexs]:\n",
        "                if len(feat.size()) == 1:\n",
        "                    aggregate_feats.append(feat.view(1, -1))\n",
        "                else:\n",
        "                    aggregate_feats.append(torch.max(feat,0)[0].view(1, -1))\n",
        "            aggregate_feats = torch.cat(aggregate_feats, 0)\n",
        "\n",
        "        return aggregate_feats"
      ],
      "metadata": {
        "id": "Jvc9W0RZfWUO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def example_with_siap(adj_matrix, raw_features, labels):\n",
        "    \"\"\"\n",
        "    Example of using GNNs with the SIAP dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - adj_matrix: Sparse adjacency matrix (csr_matrix) for the graph.\n",
        "    - raw_features: Feature matrix (numpy array).\n",
        "    - labels: Target labels (numpy array).\n",
        "    \"\"\"\n",
        "    # Ensure inputs are in correct format\n",
        "    assert isinstance(adj_matrix, csr_matrix), \"adj_matrix must be a scipy.sparse.csr_matrix\"\n",
        "    assert isinstance(raw_features, np.ndarray), \"raw_features must be a numpy array\"\n",
        "    assert isinstance(labels, np.ndarray), \"labels must be a numpy array\"\n",
        "\n",
        "    \"\"\"\n",
        "    Example of using GraphSAGE for supervised learning.\n",
        "    \"\"\"\n",
        "    print(\"Training supervised GNN with GraphSAGE...\")\n",
        "    gnn = GNN(adj_matrix, features=raw_features, labels=labels, supervised=True, model='graphsage', device='cpu', print_progress=False)\n",
        "    # Train the model\n",
        "    gnn.fit()\n",
        "    # Make predictions with the built-in MLP classifier and evaluate\n",
        "    preds = gnn.predict()\n",
        "    f1 = f1_score(labels, preds, average='micro')\n",
        "    print(f'F1 score for supervised learning on SIAP dataset: {f1:.4f}')\n",
        "    embs = gnn.generate_embeddings()\n",
        "\n",
        "    \"\"\"\n",
        "    Example of using Graph Attention Network (GAT) for unsupervised learning.\n",
        "    \"\"\"\n",
        "    print(\"Training unsupervised GNN with GAT...\")\n",
        "    gnn = GNN(adj_matrix, features=raw_features, supervised=False, model='gat', device='cpu')\n",
        "    # Train the model\n",
        "    gnn.fit()\n",
        "    # Get the node embeddings with the trained GAT\n",
        "    embs = gnn.generate_embeddings()\n",
        "    # Evaluate the embeddings with logistic regression\n",
        "    lr = LogisticRegression(penalty='l2', random_state=0, solver='liblinear')\n",
        "    preds = lr.fit(embs, labels).predict(embs)\n",
        "    f1 = f1_score(labels, preds, average='micro')\n",
        "    print(f'F1 score for unsupervised learning on SIAP dataset: {f1:.4f}')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage with preloaded data\n",
        "    # Assuming `adj_matrix`, `raw_features`, and `labels` are already defined\n",
        "    example_with_siap(adj_matrix, raw_features, labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "AUrNSwbsqIzX",
        "outputId": "265e135e-c3aa-4131-f673-68bc3af093fb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'raw_features' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-a2af1effaeaa>\u001b[0m in \u001b[0;36m<cell line: 48>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# Example usage with preloaded data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Assuming `adj_matrix`, `raw_features`, and `labels` are already defined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mexample_with_siap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'raw_features' is not defined"
          ]
        }
      ]
    }
  ]
}